{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import scipy.stats as ss\n",
    "from collections import Counter\n",
    "from mlxtend.evaluate import confusion_matrix\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import  train,test, and label dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"D:/Semester 7/Ml_Project/Training_set_values.csv\")\n",
    "test = pd.read_csv(\"D:/Semester 7/Ml_Project/Test_set_values.csv\")\n",
    "train_label = pd.read_csv(\"D:/Semester 7/Ml_Project/Training_set_labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot bar chart for label set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status_group = train_label['status_group'].value_counts().to_dict()\n",
    "\n",
    "type_ = list(status_group.keys())\n",
    "values = list(status_group.values())\n",
    "  \n",
    "fig = plt.figure(figsize = (10, 5))\n",
    " \n",
    "# creating the bar plot\n",
    "plt.bar(type_, values,\n",
    "        width = 0.4)\n",
    " \n",
    "plt.xlabel(\"status_group\")\n",
    "plt.ylabel(\"amount\")\n",
    "plt.show()\n",
    "\n",
    "list_value = list(status_group.values())\n",
    "list_value = [float(i)/sum(list_value) for i in list_value]\n",
    "list_key = list(status_group.keys())\n",
    "for i in range(len(list_value)):\n",
    "    print(str(list_key[i])+\": \"+str(list_value[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### therefore dataset is imbalanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### find duplicate rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "col_except_id = list(train.columns)[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of duplicated rows in train set: \" + str(train.duplicated(subset=col_except_id).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_indexes = train.loc[train.duplicated(subset=col_except_id), :]\n",
    "duplicate_indexes = list(duplicate_indexes.index)\n",
    "print(duplicate_indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### removing identical rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of rows in train set: \"+ str(len(train.index)))\n",
    "print(\"Number of rows in test set: \"+ str(len(test.index)))\n",
    "print(\"Number of rows in train labels: \"+ str(len(train_label.index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop_duplicates(subset=col_except_id)\n",
    "train_label = train_label.drop(train_label.index[duplicate_indexes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of rows in train set after removing duplicates: \"+ str(len(train.index)))\n",
    "print(\"Number of rows in train label set after removing duplicates: \"+ str(len(train_label.index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clarify whether identical rows are removed or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of duplicated rows in train set: \" + str(train.duplicated(subset=col_except_id).sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find columns which contain NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_col_with_NaN = train.columns[train.isna().any()].tolist()\n",
    "test_col_with_NaN = test.columns[test.isna().any()].tolist()\n",
    "\n",
    "print(\"columns which contain NaN in train set: \" + str(train_col_with_NaN))\n",
    "print(\"columns which contain NaN in test set: \" + str(test_col_with_NaN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### remove NaN values from identified columns using mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in train_col_with_NaN:\n",
    "    train[col].fillna(train[col].mode()[0], inplace = True)\n",
    "    test[col].fillna(test[col].mode()[0], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clarify whether NaN values are replced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.columns[train.isna().any()].tolist()\n",
    "test.columns[test.isna().any()].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change values less than 25 in funder and installer column to 'others'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = train['funder'].value_counts().to_dict()\n",
    "counts_ = train['installer'].value_counts().to_dict()\n",
    "for key in counts:\n",
    "    if (counts[key]<=25):\n",
    "        train[\"funder\"].replace({key: \"others\"}, inplace=True)\n",
    "\n",
    "        \n",
    "for key in counts_:\n",
    "    if (counts_[key]<=25):\n",
    "        train['installer'].replace({key: \"others\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exatract month and year from \"date_recorded\" column and create new columns called month and year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['month']=pd.to_datetime(train.date_recorded).dt.month\n",
    "test['month']=pd.to_datetime(test.date_recorded).dt.month\n",
    "\n",
    "train['year']=pd.to_datetime(train.date_recorded).dt.year\n",
    "test['year']=pd.to_datetime(test.date_recorded).dt.year\n",
    "\n",
    "train = train.drop(['date_recorded'], axis = 1)\n",
    "test = test.drop(['date_recorded'], axis = 1)\n",
    "col_except_id.remove('date_recorded')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### perform log normalization for population column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.population = train.population.apply(lambda x: np.log10(x+1))\n",
    "test.population = test.population.apply(lambda x: np.log10(x+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convert 0 value in installer column to \"others\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ins = train['installer'].unique()\n",
    "ins = list(ins)\n",
    "ins.sort()\n",
    "print(ins)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.installer.replace(to_replace=(None,'0'), value = \"others\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['construction_year']=pd.to_numeric(train['construction_year'])\n",
    "test['construction_year']=pd.to_numeric(test['construction_year'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convert 0 value in funder column to \"others\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "funder = train['funder'].unique()\n",
    "funder = list(funder)\n",
    "funder.sort()\n",
    "funder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.funder.replace(to_replace=(None, '0'), value = \"others\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.loc[:, 'scheme_management'].value_counts(dropna=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.scheme_management.replace(to_replace=('None'), value = \"others\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.loc[:, 'scheme_management'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calculate uncertainty coefficient for identify most realted feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional_entropy(x,y):\n",
    "    # entropy of x given y\n",
    "    y_counter = Counter(y)\n",
    "    xy_counter = Counter(list(zip(x,y)))\n",
    "    total_occurrences = sum(y_counter.values())\n",
    "    entropy = 0\n",
    "    for xy in xy_counter.keys():\n",
    "        p_xy = xy_counter[xy] / total_occurrences\n",
    "        p_y = y_counter[xy[1]] / total_occurrences\n",
    "        entropy += p_xy * math.log(p_y/p_xy)\n",
    "    return entropy\n",
    "\n",
    "def theil_u(x,y):\n",
    "    s_xy = conditional_entropy(x,y)\n",
    "    x_counter = Counter(x)\n",
    "    total_occurrences = sum(x_counter.values())\n",
    "    p_x = list(map(lambda n: n/total_occurrences, x_counter.values()))\n",
    "    s_x = ss.entropy(p_x)\n",
    "    if s_x == 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return (s_x - s_xy) / s_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theilu = pd.DataFrame(index=['class'],columns=train.columns)\n",
    "columns = train.columns\n",
    "for j in range(0,len(columns)):\n",
    "    u = theil_u(train_label['status_group'].tolist(),train[columns[j]].tolist())\n",
    "    theilu.loc[:,columns[j]] = u\n",
    "theilu.fillna(value=np.nan,inplace=True)\n",
    "plt.figure(figsize=(20,1))\n",
    "sns.heatmap(theilu,annot=True,fmt='.2f')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### remove less related columns from train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete_columns = ['region', 'recorded_by', 'extraction_type_class', 'management_group', 'payment', 'quality_group', 'source_type', 'source_class', 'waterpoint_type_group', 'payment_type', 'quantity_group']\n",
    "\n",
    "# delete_columns = ['date_recorded','wpt_name','num_private','subvillage','lga','ward','recorded_by','extraction_type_group','extraction_type','scheme_name','management','waterpoint_type_group','source','source_class','quantity_group','quality_group','payment_type']\n",
    "\n",
    "# for ele in delete_columns:\n",
    "    \n",
    "#     print(ele+\": \"+str(train[ele].isnull().values.any()))\n",
    "#     train = train.drop([ele], axis=1)\n",
    "#     test = test.drop([ele], axis=1)\n",
    "    \n",
    "# train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label set encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_classes = ['non functional', 'functional needs repair', 'functional']\n",
    "encoded_label = dict(zip(data_classes, range(0,3,1)))\n",
    "train_label['status_group'] = train_label['status_group'].map(encoded_label, na_action='ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do target encoding for non-numerical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from category_encoders import TargetEncoder\n",
    "from category_encoders import LeaveOneOutEncoder\n",
    "\n",
    "\n",
    "## create duplicate dataframe of train and test for catboost classification\n",
    "train_ = train\n",
    "test_ = test\n",
    "\n",
    "train_encode = train_.sample(frac=0.25, random_state=42)\n",
    "train_label_encode = train_label.loc[train_encode.index]\n",
    "\n",
    "column_with_str = []\n",
    "for col in col_except_id:\n",
    "    if (type(train[col][0]) is str ):\n",
    "        column_with_str.append(col)\n",
    "\n",
    "# encoder = LeaveOneOutEncoder(sigma=0.05, random_state=42)\n",
    "encoder = TargetEncoder()\n",
    "    \n",
    "encoder = encoder.fit(train_encode, train_label_encode['status_group'])\n",
    "train_ = encoder.transform(train_)\n",
    "test_ = encoder.transform(test_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convert boolean value columns to 0's and 1's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_.permit = train_.permit.astype(bool).astype(int)\n",
    "train_.public_meeting = train_.public_meeting.astype(bool).astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform onehot encoding for permit and public_meeting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ = pd.get_dummies(train_, columns = ['permit', 'public_meeting'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### removing 'id' column from both train_ and train_label set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = train_label.drop(['id'], axis=1)\n",
    "train_ = train_.drop(['id'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use SMOTE to handle imblanced train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "oversample = SMOTE(sampling_strategy = 'auto', n_jobs = -1, random_state=42)\n",
    "X, y = oversample.fit_resample(train_, train_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split train data set to X_train and X_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, stratify= y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization and classify using XGB classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBClassifier(objective = 'multi:softmax', booster = 'gbtree', nrounds = 'min.error.idx',\n",
    "                      num_class = 3, maximize = False, eval_metric = 'merror', eta = .1,\n",
    "                      max_depth = 16, colsample_bytree = .4, n_jobs = -1, random_state=42)\n",
    "\n",
    "X_train = ((X_train-X_train.min())/(X_train.max()-X_train.min()))\n",
    "X_test = ((X_test-X_test.min())/(X_test.max()-X_test.min()))\n",
    "\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict using X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = confusion_matrix(y_test['status_group'], predictions)\n",
    "plot_confusion_matrix(confusion_matrix)\n",
    "plt.show()\n",
    "\n",
    "# y_test.head()\n",
    "# predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = KFold(n_splits=10, random_state=42, shuffle=True)\n",
    "scores = cross_val_score(model, X_train, y_train, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "print('Accuracy: %.3f (%.3f)' % (np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convert boolean values in public_meeting and permit of test set to 0's and 1's "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_.permit = test_.permit.astype(bool).astype(int)\n",
    "test_.public_meeting = test_.public_meeting.astype(bool).astype(int)\n",
    "\n",
    "t = test_['id']\n",
    "test_ = test_.drop([\"id\"], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### onehot encoding for permit and public_meeting of test_ set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ = pd.get_dummies(test_, columns = ['permit', 'public_meeting'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict values for  test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_=((test_- test_.min())/(test_.max() - test_.min()))\n",
    "test_predictions = model.predict(test_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create csv file for output predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = test_predictions.tolist()\n",
    "list_to_csv = []\n",
    "t = list(t)\n",
    "for i  in range(len(t)):\n",
    "    if (test_predictions[i] == 0):\n",
    "        list_to_csv.append([t[i], 'non functional'])\n",
    "    elif (test_predictions[i] == 1):\n",
    "        list_to_csv.append([t[i], 'functional needs repair'])\n",
    "    else:\n",
    "        list_to_csv.append([t[i], 'functional'])\n",
    "\n",
    "to_csv = pd.DataFrame(list_to_csv, columns=['id', 'status_group'])\n",
    "to_csv.to_csv('submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(to_csv.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CatBoost classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = []\n",
    "for ele in train.columns:\n",
    "    if(type(train[ele][0]) is str):\n",
    "        cat_features.append(ele)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.permit = train.permit.astype(bool).astype(int)\n",
    "train.public_meeting = train.public_meeting.astype(bool).astype(int)\n",
    "train = pd.get_dummies(train, columns = ['permit', 'public_meeting'])\n",
    "\n",
    "test.permit = test.permit.astype(bool).astype(int)\n",
    "test.public_meeting = test.public_meeting.astype(bool).astype(int)\n",
    "test = pd.get_dummies(test, columns = ['permit', 'public_meeting'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_label = train_label.drop(['id'], axis=1)\n",
    "train = train.drop(['id'], axis=1)\n",
    "\n",
    "\n",
    "t = test['id']\n",
    "test = test.drop([\"id\"], axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_catboost, X_test_catboost, y_train_catboost, y_test_catboost = train_test_split(train,train_label, test_size=0.2, stratify= train_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "\n",
    "clf = CatBoostClassifier(\n",
    "     max_ctr_complexity=5,\n",
    "     task_type = 'CPU',\n",
    "    iterations=10000,\n",
    "    eval_metric='AUC',\n",
    "    od_type='Iter',\n",
    "    od_wait=500,\n",
    "    cat_features = cat_features,\n",
    "    verbose=False\n",
    "    \n",
    ")\n",
    "\n",
    "\n",
    "clf.fit(X_train_catboost, y_train_catboost, \n",
    "        cat_features=cat_features, \n",
    "        eval_set=(X_test_catboost, y_test_catboost), \n",
    "        verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = clf.predict(data=X_test_catboost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test_catboost, pred)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testpredictions = clf.predict(test)\n",
    "# train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = testpredictions.tolist()\n",
    "list_to_csv = []\n",
    "t = list(t)\n",
    "for i  in range(len(t)):\n",
    "    if (test_predictions[i] == 0):\n",
    "        list_to_csv.append([t[i], 'non functional'])\n",
    "    elif (test_predictions[i] == 1):\n",
    "        list_to_csv.append([t[i], 'functional needs repair'])\n",
    "    else:\n",
    "        list_to_csv.append([t[i], 'functional'])\n",
    "\n",
    "to_csv = pd.DataFrame(list_to_csv, columns=['id', 'status_group'])\n",
    "to_csv.to_csv('submission_catboost.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
